import sys
import os
sys.path.append(os.path.dirname(__file__))
import torch
import math
from model import FedModel
#from .drive import drive_compress, drive_decompress, drive_plus_compress, drive_plus_decompress
#from .eden.eden import eden_builder
import numpy as np
from typing import List
from scipy.stats import norm
import copy
import collections
from pathlib import Path
import timm
from quantization.lsq_layer import QuantAct, QuantConv2d, QuantLinear, QuantMultiHeadAct, QuantMuitiHeadLinear, QuantMuitiHeadLinear_in
import collections

@torch.no_grad()
def initialize_quantization(data_loader, model, device, sample_iters=5):
    for name, m in model.named_modules():
        if (isinstance(m, QuantLinear) or isinstance(m, QuantConv2d) or isinstance(m, QuantMuitiHeadLinear) or isinstance(m, QuantMuitiHeadLinear_in)) and m.alpha is not None:
            print(f"initialize the weight scale for module {name}")
            m.initialize_scale(device)

    # switch to evaluation mode
    model.eval()
    n = 0
    for batch_idx, (images, target) in enumerate(data_loader):
        n += 1
        if n > sample_iters:
            break
        images = images.to(device)
        output = model(images)
    for name, m in model.named_modules():
        if (isinstance(m, QuantAct) or isinstance(m, QuantMultiHeadAct)) and m.alpha is not None:
            print(f"initialize the activation scale for module {name}")
            m.initialize_scale_offset(device)
    return

class Server:
    """
        Class for the central authority, i.e., the server, that coordinates the federated process.
    """

    def __init__(self, test_data, drop, num_classes, embed_dim, transformer_depth, transformer_head, mlp_dim, img_size, lambda1, temperature, model_rate=1, device=None):
        
        self.device = device
        self.img_size = img_size
        self.training_mode = "mask" 
        self.global_model = FedModel(drop, num_classes , embed_dim , transformer_depth , transformer_head, mlp_dim ,img_size, 
                                     lambda1, temperature, model_rate=1, device=device) 
        self.round_global_weights = []
        self.round_global_weights.append(self.global_model.get_weights())
        self.current_sample_clients = []   #å½“å‰è½®å®¢æˆ·ç«¯çš„åˆ—è¡?
        self.current_training_clients = []  #ç›®å‰è¿˜åœ¨è®­ç»ƒçš„å®¢æˆ·ç«¯åˆ—è¡¨
        self.current_trained_clients = []   #ç›®å‰å·²ç»è®­ç»ƒå®Œçš„å®¢æˆ·ç«?
        self.current_aggregate_clients = []  #å½“å‰è½®è¦èšåˆçš„å®¢æˆ·ç«¯åˆ—è¡¨
        self.current_idle_clients = []
        self.stale_steps = 0
        self.update_number = 0
        self.stale_ths = 10
        self.n_round = 0
    
    def set_current_round(self, n_round):
        self.n_round = n_round
    

    def sample_clients(self):
        """
            Sample clients at each round (now uniformly at random, can be changed).
        """
        #ä»ä¸Šä¸€è½®ä¸­è¢«èšåˆçš„å®¢æˆ·ç«¯ä»¥åŠä¸Šä¸€è½®é—²æ•£çš„å®¢æˆ·ç«¯ä¸­é€‰æ‹©
        n_samples = math.floor(self.frac * self.num_users)
        tmp = self.current_idle_clients
        self.current_sample_clients = list(np.random.choice(tmp,size=min(len(tmp),n_samples), replace=False))
        self.current_idle_clients = list(set(tmp) - set(self.current_sample_clients))
        return self.current_sample_clients
    
    def set_current_training_clients(self):
        self.current_training_clients += self.current_sample_clients

    def set_asynchronous_aggregation_clients(self):
        total_client_num = len(self.current_training_clients)
        low_client_num = math.floor(total_client_num * 0.5)
        aggregate_client_num = np.random.randint(low_client_num, total_client_num)
        '''self.current_aggregate_clients = list(np.random.choice(self.current_training_clients, 
                                                               size=aggregate_client_num, replace=False))'''
        self.current_aggregate_clients = self.current_sample_clients  #TODO
        return self.current_aggregate_clients
    
    #è¿™ä¸€è½®æ²¡æœ‰è¢«èšåˆçš?
    def set_continue_training_clients(self):
        self.current_training_clients = list(set(self.current_training_clients) - set(self.current_aggregate_clients))

    def set_idle_clients(self):
        self.current_idle_clients += self.current_aggregate_clients   #ç°åœ¨é—²ç½®çš„åŠ ä¸Šä¸Šä¸€è½®è®­ç»ƒç»“æŸçš„
    
    #åœ¨æ¯ä¸€æ¬¡èšåˆä¹‹åè¿›è¡Œæ¨¡å‹æƒé‡ï¼ˆæ²¡æœ‰æ©ç ï¼‰çš„ä¿å­˜
    def save_global_weight(self):
        param_dict = {k: v for k, v in self.global_model.get_weights().items()}
        self.round_global_weights.append(param_dict)
        #torch.save({'weight_model_state_dict': param_dict}, self.server_model_path)   #å­˜å‚¨æœ€æ–°çš„æ¨¡å‹å‚æ•°

    def find_bitrate(self, probs, num_params):
        local_bitrate = 0
        for p in probs:
            local_bitrate += p * math.log2(1 / p)
        return local_bitrate * num_params

    
    def set_init_training_mode(self, current_epoch, mask_convergence):
        self.training_mode = "mask" if current_epoch > self.args.local_ep/2 or mask_convergence == True else "weight"
    
    
    def compute_staleness(self, client_idxs, client_gradients, recevie_round, client_model_rate, n_round):
        '''
        client_gradients_ls: å¾…èšåˆçš„å®¢æˆ·ç«¯å¯¹åº”çš„æ¢¯åº¦
        clinet_round_ls: å®¢æˆ·ç«¯æ”¶åˆ°æœåŠ¡å™¨çš„å…¨å±€å‚æ•°çš„è½®æ¬?
        client_idxs: å½“å‰è½®å¾…èšåˆçš„å®¢æˆ·ç«¯çš„id
        '''
        client_gamma_ls = []
        assert len(client_gradients) == len(recevie_round)

        for i, (client_name, client_receive_round) in enumerate(recevie_round.items()):
            latest_weights = self.round_global_weights[n_round]
            client_weights = self.round_global_weights[client_receive_round]
            
            gradients_importance = sum(torch.sum(torch.abs(v)) 
                                       for v in client_gradients[client_name].values()) / client_model_rate[client_name]
            weights_differnece = sum(torch.sum(torch.abs(latest_weights[k] - client_weights[k])) 
                                     for k in client_weights.keys())
            tmp_gamma = gradients_importance / (weights_differnece + 1)
            client_gamma_ls.append(tmp_gamma)
        
        tensor = torch.tensor(client_gamma_ls)
        norm_result = tensor / torch.norm(tensor, p=1)
        client_staleness_ls = norm_result.to(self.device)
        print(client_staleness_ls)
        return client_staleness_ls

    def compute_n_mask_samples(self, n_round): 
        """
            Return how many per-client samples, i.e., bits, the server wants to receive
            (can be a function of the round number). Now set to 1.
        """
        return 1

    def aggregate_gradients(self, client_gradients, recevie_round, n_round, model_rate):
        aggregated_weights = copy.deepcopy(self.global_model.get_weights())


        #print(client_gradients_ls[0])
        client_staleness_ls = self.compute_staleness(self.current_aggregate_clients, client_gradients, recevie_round, model_rate, n_round)
        for i, client_name in enumerate(client_gradients.keys()):
            for key, value in client_gradients[client_name].items():
                if "to_qkv.bias" not in key:   #weightæ¨¡å‹ä¸­è®¾ç½®äº†to_qkvçš„biasä¸ºfalseï¼Œä½†æ˜¯maskä¸­æ— æ³•è¿›è¡Œè®¾ç½?
                    aggregated_weights[key] += client_staleness_ls[i] * value
        
        self.global_model.set_weights(aggregated_weights)
        self.save_global_weight()


    def broadcast_model(self, sampled_clients, n_round):
        """
            Send the global updated model to the clients. 
        """
        for client in sampled_clients:
            self.clients_list[client].set_local_training_mode(self.training_mode)
            #self.clients_list[client].set_local_weights(self.all_client_model_list[client].get_weights())
            self.clients_list[client].set_local_weights(self.global_model.get_weights())
            self.clients_list[client].set_local_round(n_round)
